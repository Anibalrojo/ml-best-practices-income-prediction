{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.0 - Boosting Ensemble Analysis\n",
        "\n",
        "## Objective\n",
        "\n",
        "This notebook performs a deep dive into various boosting algorithms to see if they can outperform the `RandomForest` baseline. We will compare `AdaBoost`, `XGBoost`, `LightGBM`, and `CatBoost`.\n",
        "\n",
        "All models in this notebook are tree-based, so we will use `Label Encoding` for categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path to allow imports\n",
        "sys.path.append(os.path.join(os.path.abspath(''), '..', 'src'))\n",
        "\n",
        "from data.make_dataset import load_data\n",
        "from features.build_features import split_features_target, label_encode_features, split_data\n",
        "from models.train_model import train_and_evaluate, save_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n",
        "\n",
        "Load the cleaned data and prepare it for tree-based models using `Label Encoding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_data('../data/processed/adult_cleaned.csv')\n",
        "X, y = split_features_target(df)\n",
        "\n",
        "X_le = label_encode_features(X)\n",
        "X_train, X_test, y_train, y_test = split_data(X_le, y)\n",
        "\n",
        "print(\"Data prepared for tree-based models.\")\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train and Evaluate Ensemble Models\n",
        "\n",
        "We will iterate through our list of boosting models (and `RandomForest` as a baseline), train each one, and store the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_to_compare = [\"RandomForest\", \"AdaBoost\", \"XGBoost\", \"LightGBM\", \"CatBoost\"]\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    print(f\"--- Training {model_name} ---\")\n",
        "    model, metrics = train_and_evaluate(X_train, y_train, X_test, y_test, model_name)\n",
        "    results[model_name] = metrics\n",
        "    trained_models[model_name] = model\n",
        "    print(f\"ROC-AUC: {metrics['ROC-AUC']:.4f}\\n\")\n",
        "\n",
        "# Convert results to a DataFrame for display\n",
        "results_df = pd.DataFrame(results).round(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compare Results and Conclude\n",
        "\n",
        "Now we can display the performance metrics for all the trained models in a single table and visualize the key metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Comparison of Ensemble Models:\")\n",
        "print(results_df)\n",
        "\n",
        "# Plot F1-Score and ROC-AUC for comparison\n",
        "results_df.T[['F1-Score', 'ROC-AUC']].plot(\n",
        "    kind='bar',\n",
        "    figsize=(12, 6),\n",
        "    title=\"Comparison of F1-Score and ROC-AUC\"\n",
        ")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.savefig('../reports/figures/2.0_ensemble_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Best Performing Model\n",
        "\n",
        "Finally, we select the model with the highest `ROC-AUC` score from our comparison and save it to the `models/` directory. This represents the best model found for this prediction task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the save path relative to the notebook's location\n",
        "models_dir = '../models/'\n",
        "\n",
        "# Find the best model name from the results\n",
        "best_model_name = results_df.T['ROC-AUC'].idxmax()\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"Best performing model is: {best_model_name} with ROC-AUC of {results_df.loc['ROC-AUC', best_model_name]}\")\n",
        "\n",
        "# Construct the full path and save the model\n",
        "save_path = os.path.join(models_dir, 'best_performing_model.joblib')\n",
        "save_model(best_model, save_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
