# Income Prediction and Model Analysis

This project serves as a comprehensive case study on predicting individual income from the "Adult Income" dataset. It demonstrates how to structure a modern data science project by refactoring monolithic notebooks into a modular codebase and applying key software engineering principles for reproducibility and maintainability.

## Project Structure

The repository is organized using a standard data science project structure to ensure modularity and reproducibility.

```
├── data
│   ├── processed/
│   │   └── adult_cleaned.csv   <- The cleaned dataset generated by the EDA notebook.
│   └── raw/                    <- Raw data would go here (empty as we fetch from OpenML).
│
├── models/                     <- Saved models would go here.
│
├── notebooks/
│   ├── 0.1-eda-and-data-cleaning.ipynb           <- Initial data exploration and cleaning.
│   ├── 1.0-baseline-model-comparison.ipynb       <- Compares baseline Linear and Ensemble models.
│   ├── 2.0-boosting-ensemble-analysis.ipynb      <- Deep dive into boosting algorithms.
│   └── 3.0-regularized-regression-analysis.ipynb <- Analysis of regularization on linear models.
│
├── reports/
│   └── figures/                <- Saved plots and figures would go here.
│
├── src/
│   ├── __init__.py
│   ├── data/
│   │   └── make_dataset.py     <- Scripts to load and process data.
│   ├── features/
│   │   └── build_features.py   <- Scripts for feature engineering.
│   ├── models/
│   │   └── train_model.py      <- Scripts for training and evaluating models.
│   └── visualization/
│       └── visualize.py        <- Scripts for reusable visualizations.
│
├── requirements.txt            <- The project's Python dependencies.
└── README.md                   <- This file.
```

## How to Run

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd Kib-Module-5
    ```

2.  **Set up a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Run the notebooks:**
    It is crucial to run the notebooks in order, as they depend on each other.
    
    -   **Start with `0.1-eda-and-data-cleaning.ipynb`**. This notebook must be run first to generate the `adult_cleaned.csv` file in the `data/processed` directory, which is required by all other notebooks.
    -   Afterward, you can run the other notebooks (`1.0`, `2.0`, and `3.0`) in any order to explore the different analyses.

## Notebooks Overview

-   **`0.1-eda-and-data-cleaning.ipynb`**: Performs a full exploratory data analysis on the raw dataset, handles missing values, cleans data, and saves the final processed dataset.
-   **`1.0-baseline-model-comparison.ipynb`**: Establishes the project's performance baselines. It compares `ElasticNet` against `RandomForest` and saves the better of the two (based on `ROC-AUC`) as `models/baseline_model.joblib`.
    -   **Note on Baseline Choice**: We chose `ElasticNet` as the linear baseline instead of a standard `LinearRegression` because it includes regularization. This is a more robust and realistic choice for real-world applications, as it helps prevent overfitting and provides a stronger benchmark.
-   **`2.0-boosting-ensemble-analysis.ipynb`**: Compares various advanced boosting algorithms (`AdaBoost`, `XGBoost`, `LightGBM`, `CatBoost`) against the `RandomForest` baseline to find the highest-performing model. It saves the best model (based on `ROC-AUC`) as `models/best_performing_model.joblib`.
-   **`3.0-regularized-regression-analysis.ipynb`**: Provides an interpretive analysis of regularization. By treating the task as a regression problem, it visually demonstrates the impact of `L1` (Lasso) and `L2` (Ridge) regularization on model coefficients compared to a standard `LinearRegression` model. No model is saved from this notebook, as its primary purpose is analysis, not producing a deployable model.

## Software Engineering Best Practices

Beyond the data science analysis, this project allowed me to learn and implement key software engineering practices that are crucial for creating robust, reproducible, and maintainable machine learning projects.

-   **Code Modularity:** Instead of monolithic notebooks with repeated code, the project is broken into a `src/` directory. Core logic for data loading, feature engineering, and model training is centralized into Python scripts.
-   **Function Imports:** The notebooks are kept clean and focused on analysis by importing their core functionality from the `src` scripts. This makes the workflow clear and avoids code duplication.
-   **Clear Project Structure:** The repository follows a standard data science layout. This separation of concerns (e.g., `data/`, `notebooks/`, `src/`, `models/`) makes the project easy for anyone to understand, navigate, and contribute to.
-   **Automated Model Persistence:** The analysis notebooks don't just find the best model; they programmatically save the trained model object (`.joblib` file) to a designated `models/` directory. This is a crucial step for operationalizing models and making them available for future use without retraining.
-   **Dependency Management:** All required Python packages are listed in `requirements.txt`. This allows any user to create a perfectly replicated environment with a single command (`pip install -r requirements.txt`), ensuring the code runs correctly everywhere.

## Future Improvements

This project can be expanded with additional features to make it even more robust and comprehensive. Planned future improvements include:

-   **Unit Testing:** Implementing a test suite (e.g., using `pytest`) to validate the functions in the `src/` directory. This would ensure that the data processing, feature engineering, and model training logic is correct and reliable.
-   **Expanded Model Comparison:** Incorporating a wider variety of models into the analysis, such as Support Vector Machines (SVMs), Gaussian Naive Bayes, and simple neural networks, to provide a more exhaustive performance comparison.